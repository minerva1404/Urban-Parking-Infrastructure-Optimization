# ğŸ…¿ï¸ Urban Parking Infrastructure Optimization

## Real-Time HDB Carpark Streaming Pipeline (Kafka + Spark | Bronze â†’ Silver â†’ Gold)

---

## ğŸ“Œ Project Description

This project implements a production-style, real-time data pipeline for Singapore HDB carpark infrastructure data using Kafka and Apache Spark Structured Streaming.

We ingest live data from data.gov.sg, stream it through Kafka, and progressively refine it across Bronze, Silver, and Gold layersâ€”a classic Lakehouse architecture that slaps in interviews.

### ğŸ”¥ Key Features
- Real public government API ingestion
- Kafka-based event streaming
- Event-time aware processing
- Spark Structured Streaming transformations
- Entity-level Gold aggregations
- Infrastructure scoring logic
- Fault tolerance via checkpoints

This pipeline is designed to simulate real-world urban analytics systems used in smart cities, mobility platforms, and infra optimization teams.

---

## ğŸ—ï¸ Architecture Overview

data.gov.sg API
â†“
Kafka Producer
â†“
Kafka Topic (Bronze)
â†“
Spark Structured Streaming (Silver)
â†“
Cleaned JSON Storage
â†“
Spark Structured Streaming (Gold)
â†“
Aggregated Parquet (Analytics-Ready)

Bronze = raw events  
Silver = cleaned, typed, validated  
Gold = analytics + scoring layer  

Simple. Scalable. Recruiter-friendly.

---

## âš™ï¸ Installation Instructions

### 1ï¸âƒ£ Prerequisites

Make sure you have the following installed:
- Python 3.9+
- Apache Kafka
- Apache Spark 3.5+
- Java 8 / 11
- Git

---

### 2ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/Urban-Parking-Infrastructure-Optimization.git
cd Urban-Parking-Infrastructure-Optimization
```

â¸»

3ï¸âƒ£ Install Python Dependencies

```python
pip install kafka-python requests pyspark
```

â¸»

4ï¸âƒ£ Start Kafka
### Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

### Start Kafka Broker
bin/kafka-server-start.sh config/server.properties
â¸»

5ï¸âƒ£ Create Kafka Topic

bin/kafka-topics.sh --create \
  --topic hdb_carpark_bronze \
  --bootstrap-server localhost:9092 \
  --partitions 1 \
  --replication-factor 1


â¸»

â–¶ï¸ Usage Examples

ğŸš€ Step 1: Start Kafka Producer (Bronze Ingestion)

python producer.py

What happens:
	â€¢	Pulls HDB carpark data from data.gov.sg
	â€¢	Adds event_time + source
	â€¢	Streams records into Kafka every 5 seconds
	â€¢	Prints each event (full observability, no vibes-based debugging)

â¸»

ğŸ‘‚ Step 2: (Optional) Run Kafka Consumer

python consumer.py

Purpose:
	â€¢	Validate Kafka messages
	â€¢	Inspect partitions, offsets, and event-time
	â€¢	Useful for debugging and demos

â¸»

ğŸ§ª Step 3: Run Silver Streaming Job

spark-submit silver.py

Silver Layer does:
	â€¢	Parses Kafka JSON
	â€¢	Enforces schema
	â€¢	Casts numeric fields
	â€¢	Filters invalid records
	â€¢	Writes clean JSON to disk
	â€¢	Uses checkpointing for recovery

â¸»

ğŸ† Step 4: Run Gold Streaming Job

spark-submit gold.py

Gold Layer produces:
	â€¢	Entity-level aggregations per car park
	â€¢	Infrastructure KPIs:
	â€¢	Average decks
	â€¢	Gantry height stats
	â€¢	Night/free parking counts
	â€¢	Infrastructure Score
	â€¢	Writes analytics-ready Parquet

This is the layer dashboards and ML models actually care about.

â¸»

ğŸ§  Gold Layer Metrics Explained

Infrastructure Score Formula

(avg_decks * 10)
+ (avg_gantry_height * 15)
+ (night_parking_count * 2)
+ (free_parking_count * 2)

Higher score = better infrastructure capacity & accessibility.
